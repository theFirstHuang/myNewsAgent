#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæµ‹è¯•è„šæœ¬ - æµ‹è¯•arXivæ•°æ®æŠ“å–
ä¸éœ€è¦OpenAI APIå¯†é’¥ï¼Œåªæµ‹è¯•è®ºæ–‡è·å–åŠŸèƒ½
"""

import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
import json

def test_arxiv_fetch():
    """æµ‹è¯•arXiv APIæŠ“å–"""
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯• arXiv è®ºæ–‡æŠ“å–")
    print("=" * 60)
    print()

    # é…ç½®
    categories = ["cs.CL", "cs.AI"]  # LLMç›¸å…³ç±»åˆ«
    max_results = 5  # æ¯ä¸ªç±»åˆ«å–5ç¯‡
    days_lookback = 7

    all_papers = []

    for category in categories:
        print(f"ğŸ“š æ­£åœ¨æŠ“å–ç±»åˆ«: {category}")

        # æ„å»ºarXiv APIæŸ¥è¯¢
        base_url = "http://export.arxiv.org/api/query"
        query = f"cat:{category}"
        params = {
            "search_query": query,
            "start": 0,
            "max_results": max_results,
            "sortBy": "submittedDate",
            "sortOrder": "descending"
        }

        try:
            response = requests.get(base_url, params=params, timeout=10)
            response.raise_for_status()

            # è§£æXML
            root = ET.fromstring(response.content)
            ns = {'atom': 'http://www.w3.org/2005/Atom',
                  'arxiv': 'http://arxiv.org/schemas/atom'}

            entries = root.findall('atom:entry', ns)

            print(f"  âœ“ æ‰¾åˆ° {len(entries)} ç¯‡è®ºæ–‡")

            for entry in entries:
                # æå–ä¿¡æ¯
                title = entry.find('atom:title', ns).text.strip().replace('\n', ' ')

                # ä½œè€…
                authors = []
                for author in entry.findall('atom:author', ns):
                    name = author.find('atom:name', ns).text
                    authors.append(name)

                # æ‘˜è¦
                abstract = entry.find('atom:summary', ns).text.strip()

                # é“¾æ¥
                pdf_url = None
                for link in entry.findall('atom:link', ns):
                    if link.get('title') == 'pdf':
                        pdf_url = link.get('href')

                # arXiv ID
                arxiv_id = entry.find('atom:id', ns).text.split('/')[-1]

                # æ—¥æœŸ
                published = entry.find('atom:published', ns).text

                paper = {
                    'title': title[:80] + '...' if len(title) > 80 else title,
                    'authors': authors[:3],  # åªæ˜¾ç¤ºå‰3ä¸ªä½œè€…
                    'arxiv_id': arxiv_id,
                    'category': category,
                    'published': published.split('T')[0],
                    'pdf_url': pdf_url,
                    'abstract_preview': abstract[:150] + '...'
                }

                all_papers.append(paper)

        except Exception as e:
            print(f"  âœ— é”™è¯¯: {e}")
            continue

    print()
    print("=" * 60)
    print(f"ğŸ“Š æŠ“å–ç»“æœæ±‡æ€»")
    print("=" * 60)
    print(f"æ€»å…±æŠ“å–: {len(all_papers)} ç¯‡è®ºæ–‡")
    print()

    # æ˜¾ç¤ºè®ºæ–‡åˆ—è¡¨
    for i, paper in enumerate(all_papers, 1):
        print(f"\n[{i}] {paper['title']}")
        print(f"    ä½œè€…: {', '.join(paper['authors'])}")
        print(f"    ID: {paper['arxiv_id']} | ç±»åˆ«: {paper['category']}")
        print(f"    å‘è¡¨æ—¥æœŸ: {paper['published']}")
        print(f"    PDF: {paper['pdf_url']}")
        print(f"    æ‘˜è¦: {paper['abstract_preview']}")

    # ä¿å­˜åˆ°JSON
    output_file = "test_papers.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_papers, f, indent=2, ensure_ascii=False)

    print()
    print("=" * 60)
    print(f"âœ… æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print("=" * 60)

def test_huggingface_fetch():
    """æµ‹è¯•HuggingFaceæŠ“å–ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    print()
    print("=" * 60)
    print("ğŸ¤— æµ‹è¯• HuggingFace Papers æŠ“å–")
    print("=" * 60)

    url = "https://huggingface.co/papers"

    try:
        print(f"æ­£åœ¨è®¿é—®: {url}")
        response = requests.get(
            url,
            timeout=10,
            headers={'User-Agent': 'Mozilla/5.0 (Test Agent)'}
        )
        response.raise_for_status()

        print(f"âœ“ æˆåŠŸè·å–é¡µé¢ (é•¿åº¦: {len(response.content)} å­—èŠ‚)")
        print("âœ“ HuggingFace APIè¿æ¥æ­£å¸¸")

        # ä¿å­˜HTMLç”¨äºè°ƒè¯•
        with open('huggingface_test.html', 'w', encoding='utf-8') as f:
            f.write(response.text[:5000])  # åªä¿å­˜å‰5000å­—ç¬¦
        print("ğŸ“„ é¡µé¢æ ·æœ¬å·²ä¿å­˜åˆ°: huggingface_test.html")

    except Exception as e:
        print(f"âœ— é”™è¯¯: {e}")

if __name__ == "__main__":
    # æµ‹è¯•arXiv
    test_arxiv_fetch()

    # æµ‹è¯•HuggingFace
    test_huggingface_fetch()

    print()
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print()
    print("ğŸ’¡ æç¤º:")
    print("  - æŸ¥çœ‹ test_papers.json äº†è§£æŠ“å–çš„è®ºæ–‡")
    print("  - è¿™ä¸ªæµ‹è¯•ä¸éœ€è¦ä»»ä½•APIå¯†é’¥")
    print("  - åœ¨ä½ çš„æœ¬åœ°Macä¸Šè¿è¡Œå®Œæ•´ç‰ˆæœ¬ä¼šè·å¾—æ›´å¤šåŠŸèƒ½")
